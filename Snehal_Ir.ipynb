{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56fa629-5512-418a-98c0-f1ec7256966b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336319c7-68bd-4c7c-8da1-d95a1480bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleIR:\n",
    "    def __init__(self):\n",
    "        self.docs = {}\n",
    "        self.index = defaultdict(dict)\n",
    "        self.doc_freq = defaultdict(int)\n",
    "        self.doc_lengths = {}\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english')) - {'plasma', 'lens', 'maternal', 'fetal'}\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    # Tokenize and preprocess\n",
    "    def tokenize(self, text):\n",
    "        words = re.findall(r'[a-z]+', text.lower())\n",
    "        return [self.stemmer.stem(w) for w in words if w not in self.stopwords and len(w) > 1]\n",
    "\n",
    "    # Load MED documents\n",
    "    def load_docs(self, path):\n",
    "        with open(path, 'r', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        sections = content.split('.I')\n",
    "        for section in sections[1:]:\n",
    "            lines = section.strip().split('\\n')\n",
    "            if not lines: continue\n",
    "            doc_id = lines[0].strip()\n",
    "            text_lines = [line.strip() for line in lines[1:] if line.strip() and not line.startswith('.')]\n",
    "            text = ' '.join(text_lines)\n",
    "            if text:\n",
    "                self.docs[doc_id] = text\n",
    "        print(f\"Loaded {len(self.docs)} documents\")\n",
    "\n",
    "    # Load MED queries\n",
    "    def load_queries(self, path):\n",
    "        queries = {}\n",
    "        with open(path, 'r', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        sections = content.split('.I')\n",
    "        for section in sections[1:]:\n",
    "            lines = section.strip().split('\\n')\n",
    "            if not lines: continue\n",
    "            qid = lines[0].strip()\n",
    "            text_lines = [line.strip() for line in lines[1:] if line.strip() and not line.startswith('.')]\n",
    "            text = ' '.join(text_lines)\n",
    "            if text:\n",
    "                queries[qid] = text\n",
    "        print(f\"Loaded {len(queries)} queries\")\n",
    "        return queries\n",
    "\n",
    "    # Load MED relevance judgments\n",
    "    def load_qrels(self, path):\n",
    "        qrels = defaultdict(set)\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    qid, doc_id = parts[0], parts[1]\n",
    "                    doc_id_fixed = str(int(doc_id) + 1)\n",
    "                    qrels[qid].add(doc_id_fixed)\n",
    "        print(f\"Loaded relevance judgments for {len(qrels)} queries\")\n",
    "        return qrels\n",
    "\n",
    "    # Build inverted index\n",
    "    def build_index(self):\n",
    "        for doc_id, text in self.docs.items():\n",
    "            words = self.tokenize(text)\n",
    "            freq = Counter(words)\n",
    "            for word, count in freq.items():\n",
    "                self.index[word][doc_id] = count\n",
    "        for word in self.index:\n",
    "            self.doc_freq[word] = len(self.index[word])\n",
    "        # Compute document vector lengths\n",
    "        for doc_id, text in self.docs.items():\n",
    "            words = self.tokenize(text)\n",
    "            freq = Counter(words)\n",
    "            length_sq = 0\n",
    "            for word, tf in freq.items():\n",
    "                if word in self.doc_freq:\n",
    "                    idf = math.log((len(self.docs)+1)/(self.doc_freq[word]+1)) + 1\n",
    "                    tf_weight = 1 + math.log(tf)\n",
    "                    length_sq += (tf_weight * idf) ** 2\n",
    "            self.doc_lengths[doc_id] = math.sqrt(length_sq)\n",
    "        print(f\"Index built with {len(self.index)} unique terms\")\n",
    "\n",
    "    # Search\n",
    "    def search_with_base(self, query, log_base=10):\n",
    "        words = self.tokenize(query)\n",
    "        if not words: return []\n",
    "        query_freq = Counter(words)\n",
    "        query_vec = {}\n",
    "        for word, tf in query_freq.items():\n",
    "            if word in self.doc_freq:\n",
    "                idf = math.log((len(self.docs)+1)/(self.doc_freq[word]+1), log_base) + 1\n",
    "                query_vec[word] = (1 + math.log(tf)) * idf\n",
    "        query_len = math.sqrt(sum(v**2 for v in query_vec.values()))\n",
    "        scores = {}\n",
    "        for word, q_weight in query_vec.items():\n",
    "            for doc_id, tf in self.index.get(word, {}).items():\n",
    "                tf_weight = 1 + math.log(tf)\n",
    "                idf = math.log((len(self.docs)+1)/(self.doc_freq[word]+1), log_base) + 1\n",
    "                scores[doc_id] = scores.get(doc_id, 0) + (tf_weight * idf) * q_weight\n",
    "        # Cosine normalization\n",
    "        for doc_id in scores:\n",
    "            if self.doc_lengths[doc_id] > 0 and query_len > 0:\n",
    "                scores[doc_id] /= (self.doc_lengths[doc_id] * query_len)\n",
    "            else:\n",
    "                scores[doc_id] = 0\n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Evaluate MAP, MAP@30, precision at recall\n",
    "    def evaluate(self, queries, qrels, log_bases=[0.1,0.2,0.3,0.5,1.5,2,10], top_k=30):\n",
    "        recall_levels = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "        all_results = {}\n",
    "        for lb in log_bases:\n",
    "            prec_at_recall = {r:[] for r in recall_levels}\n",
    "            map_scores = []\n",
    "            map30_scores = []\n",
    "            for qid, query in queries.items():\n",
    "                relevant_docs = qrels.get(qid, set())\n",
    "                if not relevant_docs: continue\n",
    "                results_docs = self.search_with_base(query, lb)[:top_k]\n",
    "                retrieved_docs = [doc_id for doc_id,_ in results_docs]\n",
    "                # Compute precision at each recall level\n",
    "                num_rel = 0\n",
    "                precisions = []\n",
    "                for i, doc_id in enumerate(retrieved_docs):\n",
    "                    if doc_id in relevant_docs:\n",
    "                        num_rel += 1\n",
    "                        precisions.append(num_rel / (i+1))\n",
    "                        # Map precision to recall level\n",
    "                        recall = num_rel / len(relevant_docs)\n",
    "                        for r in recall_levels:\n",
    "                            if recall >= r:\n",
    "                                prec_at_recall[r].append(num_rel/(i+1))\n",
    "                # Average Precision\n",
    "                ap = sum(precisions)/len(relevant_docs) if len(relevant_docs)>0 else 0\n",
    "                map_scores.append(ap)\n",
    "                map30_scores.append(sum(precisions)/min(len(relevant_docs), top_k) if len(relevant_docs)>0 else 0)\n",
    "            # Average over queries\n",
    "            avg_prec_at_recall = {r: sum(v)/len(v) if len(v)>0 else 0 for r,v in prec_at_recall.items()}\n",
    "            avg_map = sum(map_scores)/len(map_scores) if len(map_scores)>0 else 0\n",
    "            avg_map30 = sum(map30_scores)/len(map30_scores) if len(map30_scores)>0 else 0\n",
    "            all_results[lb] = {'precision_recall': avg_prec_at_recall, 'MAP': avg_map, 'MAP@30': avg_map30}\n",
    "\n",
    "        # Display results similar to Tables 4-6\n",
    "        print(\"\\nEvaluation Summary:\")\n",
    "        for lb,res in all_results.items():\n",
    "            print(f\"\\nLog base {lb}: MAP={res['MAP']:.4f}, MAP@30={res['MAP@30']:.4f}\")\n",
    "            print(\"Recall -> Precision:\")\n",
    "            for r in recall_levels:\n",
    "                print(f\"{r:.1f}: {res['precision_recall'][r]:.3f}\", end=\" | \")\n",
    "            print()\n",
    "        # Identify best, worst, standard\n",
    "        map30_values = {lb:res['MAP@30'] for lb,res in all_results.items()}\n",
    "        best_lb = max(map30_values, key=map30_values.get)\n",
    "        worst_lb = min(map30_values, key=map30_values.get)\n",
    "        standard_lb = 10\n",
    "        print(f\"\\nBest log base (MAP@30): {best_lb}, Standard (log10): {standard_lb}, Worst: {worst_lb}\")\n",
    "        return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a662690-1d36-4353-9b20-484cb9dd71e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1033 documents\n",
      "Loaded 30 queries\n",
      "Loaded relevance judgments for 30 queries\n",
      "Index built with 8877 unique terms\n",
      "\n",
      "Evaluation Summary:\n",
      "\n",
      "Log base 0.1: MAP=0.0000, MAP@30=0.0000\n",
      "Recall -> Precision:\n",
      "0.0: 0.000 | 0.1: 0.000 | 0.2: 0.000 | 0.3: 0.000 | 0.4: 0.000 | 0.5: 0.000 | 0.6: 0.000 | 0.7: 0.000 | 0.8: 0.000 | 0.9: 0.000 | 1.0: 0.000 | \n",
      "\n",
      "Log base 0.2: MAP=0.0000, MAP@30=0.0000\n",
      "Recall -> Precision:\n",
      "0.0: 0.000 | 0.1: 0.000 | 0.2: 0.000 | 0.3: 0.000 | 0.4: 0.000 | 0.5: 0.000 | 0.6: 0.000 | 0.7: 0.000 | 0.8: 0.000 | 0.9: 0.000 | 1.0: 0.000 | \n",
      "\n",
      "Log base 0.3: MAP=0.0000, MAP@30=0.0000\n",
      "Recall -> Precision:\n",
      "0.0: 0.000 | 0.1: 0.000 | 0.2: 0.000 | 0.3: 0.000 | 0.4: 0.000 | 0.5: 0.000 | 0.6: 0.000 | 0.7: 0.000 | 0.8: 0.000 | 0.9: 0.000 | 1.0: 0.000 | \n",
      "\n",
      "Log base 0.5: MAP=0.0000, MAP@30=0.0000\n",
      "Recall -> Precision:\n",
      "0.0: 0.000 | 0.1: 0.000 | 0.2: 0.000 | 0.3: 0.000 | 0.4: 0.000 | 0.5: 0.000 | 0.6: 0.000 | 0.7: 0.000 | 0.8: 0.000 | 0.9: 0.000 | 1.0: 0.000 | \n",
      "\n",
      "Log base 1.5: MAP=0.0012, MAP@30=0.0012\n",
      "Recall -> Precision:\n",
      "0.0: 0.037 | 0.1: 0.037 | 0.2: 0.037 | 0.3: 0.037 | 0.4: 0.037 | 0.5: 0.037 | 0.6: 0.037 | 0.7: 0.037 | 0.8: 0.037 | 0.9: 0.037 | 1.0: 0.037 | \n",
      "\n",
      "Log base 2: MAP=0.0014, MAP@30=0.0014\n",
      "Recall -> Precision:\n",
      "0.0: 0.042 | 0.1: 0.042 | 0.2: 0.042 | 0.3: 0.042 | 0.4: 0.042 | 0.5: 0.042 | 0.6: 0.042 | 0.7: 0.042 | 0.8: 0.042 | 0.9: 0.042 | 1.0: 0.042 | \n",
      "\n",
      "Log base 10: MAP=0.0015, MAP@30=0.0015\n",
      "Recall -> Precision:\n",
      "0.0: 0.045 | 0.1: 0.045 | 0.2: 0.045 | 0.3: 0.045 | 0.4: 0.045 | 0.5: 0.045 | 0.6: 0.045 | 0.7: 0.045 | 0.8: 0.045 | 0.9: 0.045 | 1.0: 0.045 | \n",
      "\n",
      "Best log base (MAP@30): 10, Standard (log10): 10, Worst: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    ir = SimpleIR()\n",
    "    ir.load_docs(r\"C:\\Snehal\\ir\\med\\MED.ALL\")\n",
    "    queries = ir.load_queries(r\"C:\\Snehal\\ir\\med\\MED.QRY\")\n",
    "    qrels = ir.load_qrels(r\"C:\\Snehal\\ir\\med\\MED.REL\")\n",
    "\n",
    "    ir.build_index()\n",
    "    results = ir.evaluate(queries, qrels, top_k=30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
